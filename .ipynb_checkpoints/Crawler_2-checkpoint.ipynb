{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "from urllib.parse import parse_qs, urlparse\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I. Crawl Information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper function to extract information about Recipients of Hong Kong Special Administrative Region Honours and Awards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_data(soup, language = 'Eng'):\n",
    "    '''\n",
    "    @soup: BeautifulSoup object contains html of the website\n",
    "    @language: a string indicates whether the language is Chi or Eng\n",
    "    \n",
    "    the function extract member information on current page and return a data dictionary with below format\n",
    "    return data: [member1: {'year': }, member2: {'year': }]\n",
    "    ''' \n",
    "    data = []\n",
    "    content = soup.find_all('tr',{'valign':'top'})\n",
    "    for member in content:\n",
    "        name = member.find_all('td')[0].text\n",
    "        date = member.find_all('td')[1].text\n",
    "        # skip rows without proper name and date\n",
    "        try:\n",
    "            day, month, year = date.split('.')\n",
    "        except:\n",
    "            continue\n",
    "        date = datetime(int(year),int(month),int(day))\n",
    "        data.append({'Member':name, 'Date':datetime.date(date)})\n",
    "        \n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## English version "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract html on the first page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "eng_url = \"https://www.info.gov.hk/cml/eng/miscell/index3.htm\"\n",
    "eng_html = requests.get(eng_url)\n",
    "soup = BeautifulSoup(eng_html.text, 'html.parser')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get award Eng name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_award = soup.find('h1',{'class':'header'}).text.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get urls of next 3 pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls = []\n",
    "page_urls = soup.find('td',{'align':'center'}).find_all('a')\n",
    "for page_url in page_urls:\n",
    "    urls.append(page_url.get('href'))\n",
    "\n",
    "main_url = urlparse(eng_url).scheme + '://' + urlparse(eng_url).netloc\n",
    "paths = urlparse(eng_url).path.split('/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract member data on all pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "en_data = []\n",
    "while True: \n",
    "        \n",
    "    # extract data of current page and append to en_data\n",
    "    data = extract_data(soup, language = 'Eng') \n",
    "    en_data = en_data + data\n",
    "    \n",
    "    if i==3:\n",
    "        break\n",
    "        \n",
    "    # replace path to get url of next page, update soup\n",
    "    paths[-1] = urls[i]\n",
    "    eng_url = main_url + '/'.join(paths)\n",
    "    eng_html = requests.get(eng_url)\n",
    "    soup = BeautifulSoup(eng_html.text, 'html.parser')\n",
    "    \n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save data to Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(315, 3)\n"
     ]
    }
   ],
   "source": [
    "en_df = pd.DataFrame(en_data)\n",
    "en_df['Award'] = en_award\n",
    "en_df.columns = ['Eng Name','Award Date','Award']\n",
    "print(en_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chinese version "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To avoid Chinese decoding problem, use \"utf-8\" encoding. Extract html on the first page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "cn_url = \"https://www.info.gov.hk/cml/miscell/index3.htm\"\n",
    "cn_html = requests.get(cn_url)\n",
    "cn_html.encoding = 'utf-8'\n",
    "soup = BeautifulSoup(cn_html.text, 'html.parser')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get urls of next 3 pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls = []\n",
    "page_urls = soup.find('td',{'align':'center'}).find_all('a')\n",
    "for page_url in page_urls:\n",
    "    urls.append(page_url.get('href'))\n",
    "\n",
    "main_url = urlparse(cn_url).scheme + '://' + urlparse(cn_url).netloc\n",
    "paths = urlparse(cn_url).path.split('/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract data on all pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "cn_data = []\n",
    "while True: \n",
    "        \n",
    "    # extract data of current page and append to en_data\n",
    "    data = extract_data(soup, language = 'Eng') \n",
    "    cn_data = cn_data + data\n",
    "    \n",
    "    if i==3:\n",
    "        break\n",
    "        \n",
    "    # replace path to get url of next page, update soup\n",
    "    paths[-1] = urls[i]\n",
    "    cn_url = main_url + '/'.join(paths)\n",
    "    cn_html = requests.get(cn_url)\n",
    "    cn_html.encoding = 'utf-8'\n",
    "    soup = BeautifulSoup(cn_html.text, 'html.parser')\n",
    "    \n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save data to Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(315, 2)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cn_df = pd.DataFrame(cn_data)\n",
    "cn_df.columns = ['Chi Name','Award Date']\n",
    "cn_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merger Eng and Chi dataframes and rename columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = en_df.merge(cn_df, left_index = True, right_index = True, how='outer')\n",
    "df.drop(['Award Date_y'],axis = 1,inplace = True)\n",
    "df.columns = ['Eng Name','Award Date','Award','Chi Name']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add organization and catefory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Category'] = 'Awards & Recognitions'\n",
    "df['Organization'] = 'Justices of the Peace'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Delete (the Honourable) and award abbreviation in Eng names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete (the Honourable) in Eng Name\n",
    "df['Eng Name'] = df['Eng Name'].apply(lambda x: x[:x.find('(')] if '(' in x else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def del_abbreviation(name):\n",
    "    abbreviate = re.search('\\, [A-Z]\\.',name)\n",
    "    if abbreviate:\n",
    "        return name[:name.find(abbreviate.group())].strip()\n",
    "    else:\n",
    "        return name.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete abbreviation in Eng Name\n",
    "df['Eng Name'] = df['Eng Name'].apply(del_abbreviation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Delete abbreviation in Chi Name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Chi Name'] = df['Chi Name'].apply(lambda x: x.split('，')[0].strip() if '，' in x else x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add gender based on prefix in Chi Name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Gender'] = df['Chi Name'].apply(lambda x: 'F' if '女士' in x else \"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split prefix in Chi Name only, since there is no prefix in Eng name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_prefix_cn(name):\n",
    "    # if the name in Engslish, don't split prefix\n",
    "    en_name = re.search(r'[a-zA-Z0-9]+', name)\n",
    "    if en_name:\n",
    "        return \"\"\n",
    "    # if the name is in Chinese, return Chinese prefix\n",
    "    else:\n",
    "        prefix = re.search('(?:先生|小姐|女士|工程師|機長|博士|首席|議員|首席法官|爵士|法師|教授|醫生|大主教|副庭長|勳爵|法官|牧師)',name)\n",
    "        if prefix:\n",
    "            return prefix.group()\n",
    "        else:\n",
    "            return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Prefix CN'] = df['Chi Name'].apply(split_prefix_cn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split first and last Eng name, also split if there is preferred name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_last_name_eng(name):\n",
    "    last_name = re.search('[A-Z]+(?![a-z])',name)\n",
    "    if last_name:\n",
    "        return last_name.group()\n",
    "    else:\n",
    "        return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Last Name Eng'] = df['Eng Name'].apply(split_last_name_eng)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['First Name Eng'] = df.apply(lambda x: x['Eng Name'].replace(x['Last Name Eng'], \"\").strip(), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df['Preferred Name Eng'] = df.apply(lambda x: x['First Name Eng'].split(',')[1] if ',' in x['First Name Eng'] else \"\", axis =1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove preferred name from first name\n",
    "df['First Name Eng'] = df['First Name Eng'].apply(lambda x: x.split(',')[0] if ',' in x else x)\n",
    "# remove - at the beginning of the first name\n",
    "df['First Name Eng'] = df['First Name Eng'].apply(lambda x: x[1:] if x.find('-') == 0 else x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split first and last Chi name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_last_name_chi(name):\n",
    "    # if the name is in Chinese, return first character \n",
    "    en_name = re.search(r'[a-zA-Z0-9]+', name)\n",
    "    if en_name:\n",
    "        return split_last_name_eng(en_name.group())\n",
    "    else:\n",
    "        return name[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Last Name Chi'] = df['Chi Name'].apply(split_last_name_chi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df['First Name Chi'] = df.apply(lambda x: x['Chi Name'].replace(x['Prefix CN'], \"\")\n",
    "                               .replace(x['Last Name Chi'], \"\").strip(), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Order the columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = df[['Last Name Chi','First Name Chi','Chi Name','Prefix CN',\n",
    "         'Last Name Eng','First Name Eng','Preferred Name Eng','Eng Name',\n",
    "         'Gender','Category','Organization','Award Date','Award']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save to csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_excel('太平绅士名单.xlsx',encoding='utf_8_sig')\n",
    "df.to_csv('太平绅士名单.csv',encoding='utf_8_sig')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
